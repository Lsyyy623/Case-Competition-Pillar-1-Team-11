{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ecf5f-4191-43ee-96ae-4fe6823b7472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Batch fetch abstracts for Kellogg papers (resumable version)\n",
    "Pipeline: Crossref â†’ Semantic Scholar (incl. DOI direct lookup) â†’ Google Scholar (SerpAPI as fallback)\n",
    "\n",
    "Input file: YOUR_FILE_NAME.xlsx\n",
    "Required columns: title / authors / year / url / abstract (abstract can be empty)\n",
    "\n",
    "- Automatic resume-point: based on KEY (sha1 of title + authors + year + url)\n",
    "- Periodic autosave: *_autosave_with_abstracts.xlsx / *_autosave_need_manual.xlsx\n",
    "- 429/network errors: exponential backoff + respect Retry-After\n",
    "\"\"\"\n",
    "\n",
    "# ======ã€Configã€‘======\n",
    "INPUT_FILE        = \"YOUR_FILE_NAME.xlsx\"   # Excel file for Kellogg papers\n",
    "PROFESSOR_NAME    = None           # Filter by professor name substring; otherwise None\n",
    "S2_API_KEY        = None           # Optional: Semantic Scholar API key (not required)\n",
    "SERPAPI_KEY       = \"YOUR API KEY\"  # SerpAPI Key\n",
    "\n",
    "# Runtime parameters\n",
    "REQUEST_INTERVAL  = 1.0            # Minimum delay per request (be polite)\n",
    "TOP_K             = 3              # Candidate count for each source\n",
    "FUZZ_THRESHOLD    = 78             # change as you needed\n",
    "AUTOSAVE_EVERY    = 50             # Autosave every N rows\n",
    "\n",
    "import os, re, time, html, sys, json, math, hashlib, requests, pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Rough pattern for identifying likely \"no public abstract\" works\n",
    "LIKELY_NO_ABS_PAT = re.compile(\n",
    "    r\"(forthcoming|in\\s*press|pre[-\\s]*print|working\\s*paper|under\\s*review|submitted|ssrn|arxiv|nber)\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "# ========== Helpers ==========\n",
    "def row_key(title: str, authors: str, year, url: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Generate KEY from Kellogg columns, used for resumable dedup:\n",
    "    sha1(title + authors + year + url)\n",
    "    \"\"\"\n",
    "    base = f\"{str(title).strip()}||{str(authors).strip()}||{str(year).strip()}||{str(url).strip()}\"\n",
    "    return hashlib.sha1(base.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def jats_to_text(jats: str) -> str:\n",
    "    s = html.unescape(jats or \"\")\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def http_get(url, params=None, timeout=25, headers=None, tries=6, sleep=0.8):\n",
    "    \"\"\"Generic GET: handle 429 (respect Retry-After), 5xx (exponential backoff), retry on limited attempts.\"\"\"\n",
    "    last_status = None\n",
    "    for k in range(tries):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=timeout, headers=headers)\n",
    "            if r.status_code == 200:\n",
    "                return r\n",
    "            if r.status_code == 429:\n",
    "                ra = r.headers.get(\"Retry-After\")\n",
    "                wait = float(ra) if ra else min(sleep * (2 ** k), 64.0)\n",
    "                time.sleep(wait)\n",
    "                last_status = 429\n",
    "                continue\n",
    "            if 500 <= r.status_code < 600:\n",
    "                time.sleep(min(sleep * (k + 1), 30.0))\n",
    "                last_status = r.status_code\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "        except requests.RequestException:\n",
    "            time.sleep(min(sleep * (k + 1), 30.0))\n",
    "            last_status = \"network\"\n",
    "    raise RuntimeError(f\"GET fail: {url} | status={last_status}\")\n",
    "\n",
    "def safe_to_excel(df: pd.DataFrame, path: str):\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            df.to_excel(path, index=False)\n",
    "            return\n",
    "        except Exception:\n",
    "            time.sleep(1.0)\n",
    "\n",
    "# ========== Data Sources ==========\n",
    "def search_crossref_biblio(q, rows=TOP_K):\n",
    "    url = \"https://api.crossref.org/works\"\n",
    "    params = {\n",
    "        \"query.bibliographic\": q,\n",
    "        \"rows\": rows,\n",
    "        \"select\": \"title,abstract,issued,container-title,DOI,URL\"\n",
    "    }\n",
    "    r = http_get(url, params=params, headers={\"User-Agent\": \"abstract-fetcher/1.0\"})\n",
    "    return (r.json().get(\"message\") or {}).get(\"items\", [])\n",
    "\n",
    "# â€”â€” Semantic Scholar â€”â€”\n",
    "def s2_headers():\n",
    "    h = {\"User-Agent\": \"abstract-fetcher/1.0\"}\n",
    "    if S2_API_KEY:\n",
    "        h[\"x-api-key\"] = S2_API_KEY\n",
    "    return h\n",
    "\n",
    "def search_semantic_scholar(q, rows=TOP_K):\n",
    "    \"\"\"\n",
    "    Search by title/authors.\n",
    "    fields: title, abstract, year, venue, url, externalIds\n",
    "    \"\"\"\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\n",
    "        \"query\": q,\n",
    "        \"limit\": rows,\n",
    "        \"fields\": \"title,abstract,year,venue,url,externalIds\"\n",
    "    }\n",
    "    r = http_get(url, params=params, headers=s2_headers(), timeout=30, tries=5, sleep=1.0)\n",
    "    return (r.json() or {}).get(\"data\", []) or []\n",
    "\n",
    "def fetch_semantic_by_doi(doi: str):\n",
    "    \"\"\"Semantic Scholar: direct fetch via DOI\"\"\"\n",
    "    if not doi:\n",
    "        return None\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/DOI:{doi}\"\n",
    "    params = {\"fields\": \"title,abstract,year,venue,url,externalIds\"}\n",
    "    try:\n",
    "        r = http_get(url, params=params, headers=s2_headers(), timeout=30, tries=5, sleep=1.0)\n",
    "        return r.json()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# â€”â€” Google Scholar (SerpAPI fallback, using snippet as weak abstract) â€”â€”\n",
    "def search_google_scholar_serpapi(title: str, author_hint: str = \"\"):\n",
    "    \"\"\"Return raw SerpAPI results; best title match used as weak abstract\"\"\"\n",
    "    if not SERPAPI_KEY or SERPAPI_KEY.startswith(\"PUT_\"):\n",
    "        return None\n",
    "    url = \"https://serpapi.com/search.json\"\n",
    "    q = f'{title} {author_hint}'.strip()\n",
    "    params = {\n",
    "        \"engine\": \"google_scholar\",\n",
    "        \"q\": q,\n",
    "        \"api_key\": SERPAPI_KEY,\n",
    "        \"hl\": \"en\",\n",
    "        \"num\": 10\n",
    "    }\n",
    "    try:\n",
    "        r = http_get(url, params=params, timeout=30, headers={\"User-Agent\": \"abstract-fetcher/1.0\"}, tries=4, sleep=1.2)\n",
    "        data = r.json()\n",
    "        items = data.get(\"organic_results\", []) or []\n",
    "        return items\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def best_by_fuzz(target, items, key):\n",
    "    best, score = None, 0\n",
    "    for it in items:\n",
    "        t = key(it) or \"\"\n",
    "        s = fuzz.token_set_ratio(target, t)\n",
    "        if s > score:\n",
    "            best, score = it, s\n",
    "    return (best, score) if score >= FUZZ_THRESHOLD else (None, score)\n",
    "\n",
    "# ========== Fetch One ==========\n",
    "def fetch_one(title_guess: str, authors: str, year):\n",
    "    \"\"\"\n",
    "    Only rely on Kellogg columns: title_guess / authors / year.\n",
    "    No dependency on any 'Entry' parsing.\n",
    "    \"\"\"\n",
    "    debug = {\n",
    "        \"QueryUsed\": \"\",\n",
    "        \"CandidateTitle\": \"\",\n",
    "        \"CandidateDOI\": \"\",\n",
    "        \"WhyNoAbstract\": \"\"\n",
    "    }\n",
    "\n",
    "    # Rough guess about lacking public abstract\n",
    "    meta_text = f\"{title_guess} {authors}\"\n",
    "    likely_no_abs = bool(LIKELY_NO_ABS_PAT.search(meta_text or \"\"))\n",
    "\n",
    "    # 1) Crossref\n",
    "    q_parts = [title_guess]\n",
    "    if authors:\n",
    "        q_parts.append(authors)\n",
    "    if year:\n",
    "        q_parts.append(str(year))\n",
    "    q = \" \".join(q_parts).strip()\n",
    "\n",
    "    items = search_crossref_biblio(q)\n",
    "    debug[\"QueryUsed\"] = f\"CR-biblio:{q[:120]}\"\n",
    "\n",
    "    # Sort by closeness of year\n",
    "    if year:\n",
    "        def year_from_item(x):\n",
    "            issued = x.get(\"issued\", {}).get(\"date-parts\") or [[None]]\n",
    "            return issued[0][0] or 9999\n",
    "        items = sorted(items, key=lambda x: abs(year_from_item(x) - int(year)))\n",
    "\n",
    "    cand, score = best_by_fuzz(\n",
    "        title_guess,\n",
    "        items,\n",
    "        key=lambda x: (x.get(\"title\") or [\"\"])[0] if x.get(\"title\") else \"\"\n",
    "    )\n",
    "\n",
    "    s2_after_cr = None\n",
    "\n",
    "    if cand:\n",
    "        ctitle = (cand.get(\"title\") or [\"\"])[0] if cand.get(\"title\") else \"\"\n",
    "        doi    = cand.get(\"DOI\", \"\")\n",
    "        abs_cr = jats_to_text(cand.get(\"abstract\", \"\") or \"\")\n",
    "        debug.update({\"CandidateTitle\": ctitle, \"CandidateDOI\": doi})\n",
    "\n",
    "        if abs_cr:\n",
    "            res = {\n",
    "                \"Abstract\": abs_cr,\n",
    "                \"DOI\": doi,\n",
    "                \"URL\": cand.get(\"URL\", \"\"),\n",
    "                \"Year_Found\": (cand.get(\"issued\", {}).get(\"date-parts\") or [[None]])[0][0],\n",
    "                \"Venue\": (cand.get(\"container-title\") or [\"\"])[0] if cand.get(\"container-title\") else \"\",\n",
    "                \"Source\": \"Crossref\",\n",
    "                \"Status\": \"ok\",\n",
    "            }\n",
    "            res.update(debug)\n",
    "            return res, score\n",
    "\n",
    "        # No abstract: try Semantic Scholar via DOI\n",
    "        s2_after_cr = {\n",
    "            \"Abstract\": \"\",\n",
    "            \"DOI\": doi,\n",
    "            \"URL\": cand.get(\"URL\", \"\"),\n",
    "            \"Year_Found\": (cand.get(\"issued\", {}).get(\"date-parts\") or [[None]])[0][0],\n",
    "            \"Venue\": (cand.get(\"container-title\") or [\"\"])[0] if cand.get(\"container-title\") else \"\",\n",
    "            \"Source\": \"Crossref\",\n",
    "            \"Status\": \"no_abstract_yet\" if likely_no_abs else \"no_abstract_from_cr\",\n",
    "        }\n",
    "        s2_after_cr.update(debug)\n",
    "        try:\n",
    "            if doi:\n",
    "                s2 = fetch_semantic_by_doi(doi)\n",
    "                if s2 and (s2.get(\"abstract\") or \"\"):\n",
    "                    s2_abs = (s2.get(\"abstract\") or \"\").strip()\n",
    "                    venue  = s2.get(\"venue\", \"\") or s2_after_cr.get(\"Venue\", \"\")\n",
    "                    url    = s2.get(\"url\", \"\") or s2_after_cr.get(\"URL\", \"\")\n",
    "                    year_f = s2.get(\"year\", \"\") or s2_after_cr.get(\"Year_Found\", \"\")\n",
    "                    s2_after_cr.update({\n",
    "                        \"Abstract\": s2_abs,\n",
    "                        \"Venue\": venue,\n",
    "                        \"URL\": url,\n",
    "                        \"Year_Found\": year_f,\n",
    "                        \"Source\": \"Semantic Scholar (by DOI)\",\n",
    "                        \"Status\": \"ok\",\n",
    "                    })\n",
    "                    return s2_after_cr, score\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    time.sleep(REQUEST_INTERVAL)\n",
    "\n",
    "    # 2) Semantic Scholar (title+authors)\n",
    "    try:\n",
    "        s2_list = search_semantic_scholar(f\"{title_guess} {authors}\".strip())\n",
    "    except Exception:\n",
    "        s2_list = []\n",
    "\n",
    "    cand_s2, score_s2 = None, 0\n",
    "    for it in s2_list:\n",
    "        t = it.get(\"title\", \"\") or \"\"\n",
    "        s = fuzz.token_set_ratio(title_guess, t)\n",
    "        if s > score_s2:\n",
    "            cand_s2, score_s2 = it, s\n",
    "\n",
    "    if cand_s2:\n",
    "        abs_txt = (cand_s2.get(\"abstract\") or \"\").strip()\n",
    "        if abs_txt:\n",
    "            ext        = cand_s2.get(\"externalIds\") or {}\n",
    "            doi        = ext.get(\"DOI\", \"\") or \"\"\n",
    "            url        = cand_s2.get(\"url\", \"\") or \"\"\n",
    "            venue      = (cand_s2.get(\"venue\") or \"\") or \"\"\n",
    "            year_found = cand_s2.get(\"year\", \"\") or \"\"\n",
    "            res = {\n",
    "                \"Abstract\": abs_txt,\n",
    "                \"DOI\": doi,\n",
    "                \"URL\": url,\n",
    "                \"Year_Found\": year_found,\n",
    "                \"Venue\": venue,\n",
    "                \"Source\": \"Semantic Scholar\",\n",
    "                \"Status\": \"ok\",\n",
    "            }\n",
    "            res.update(debug)\n",
    "            return res, score_s2\n",
    "\n",
    "    time.sleep(REQUEST_INTERVAL)\n",
    "\n",
    "    # 3) Google Scholar (SerpAPI fallback)\n",
    "    try:\n",
    "        gs = search_google_scholar_serpapi(title_guess, author_hint=authors)\n",
    "        if gs:\n",
    "            cand4, score4 = best_by_fuzz(title_guess, gs, key=lambda x: x.get(\"title\", \"\"))\n",
    "            if cand4 and score4 >= FUZZ_THRESHOLD:\n",
    "                snippet = cand4.get(\"snippet\", \"\") or \"\"\n",
    "                pub_info = cand4.get(\"publication_info\", {}) or {}\n",
    "                year_found = None\n",
    "                if isinstance(pub_info, dict):\n",
    "                    yf = re.search(r\"\\b(20\\d{2}|19\\d{2})\\b\", json.dumps(pub_info))\n",
    "                    year_found = int(yf.group(1)) if yf else \"\"\n",
    "                res = {\n",
    "                    \"Abstract\": snippet,\n",
    "                    \"DOI\": \"\",\n",
    "                    \"URL\": cand4.get(\"link\", \"\") or \"\",\n",
    "                    \"Year_Found\": year_found or \"\",\n",
    "                    \"Venue\": pub_info.get(\"summary\", \"\") if isinstance(pub_info, dict) else \"\",\n",
    "                    \"Source\": \"Google Scholar (SerpAPI)\",\n",
    "                    \"Status\": \"ok-snippet\",\n",
    "                }\n",
    "                res.update(debug)\n",
    "                return res, score4\n",
    "    except Exception as e:\n",
    "        debug[\"WhyNoAbstract\"] += f\"GS-fail({type(e).__name__}); \"\n",
    "\n",
    "    # 4) No abstract found in all sources\n",
    "    final = s2_after_cr or {\n",
    "        \"Abstract\": \"\",\n",
    "        \"DOI\": \"\",\n",
    "        \"URL\": \"\",\n",
    "        \"Year_Found\": \"\",\n",
    "        \"Venue\": \"\",\n",
    "        \"Source\": \"\"\n",
    "    }\n",
    "    why = \"No abstract in CR/S2/GS\"\n",
    "    if likely_no_abs:\n",
    "        why = \"Likely no public abstract (forthcoming/pre/working)\"\n",
    "        final[\"Status\"] = \"likely_no_public_abstract\"\n",
    "    else:\n",
    "        final[\"Status\"] = final.get(\"Status\", \"no_abstract_all_sources\")\n",
    "    final.update({\"WhyNoAbstract\": why})\n",
    "    final.update(debug)\n",
    "    return final, 0\n",
    "\n",
    "# ========== Resume-point Loader ==========\n",
    "def load_done_keys(prefix: str):\n",
    "    done = set()\n",
    "    for suf in [\n",
    "        \"_with_abstracts.xlsx\",\n",
    "        \"_need_manual.xlsx\",\n",
    "        \"_autosave_with_abstracts.xlsx\",\n",
    "        \"_autosave_need_manual.xlsx\"\n",
    "    ]:\n",
    "        p = Path(f\"{prefix}{suf}\")\n",
    "        if p.exists():\n",
    "            try:\n",
    "                df = pd.read_excel(p)\n",
    "                if \"KEY\" in df.columns:\n",
    "                    done.update(df[\"KEY\"].dropna().astype(str).tolist())\n",
    "                else:\n",
    "                    df = df.copy()\n",
    "                    df[\"_KEY\"] = df.apply(\n",
    "                        lambda r: row_key(\n",
    "                            r.get(\"title\", \"\"),\n",
    "                            r.get(\"authors\", \"\"),\n",
    "                            r.get(\"year\", \"\"),\n",
    "                            r.get(\"url\", \"\")\n",
    "                        ),\n",
    "                        axis=1\n",
    "                    )\n",
    "                    done.update(df[\"_KEY\"].tolist())\n",
    "            except Exception:\n",
    "                pass\n",
    "    return done\n",
    "\n",
    "# ========== Batch main workflow ==========\n",
    "def run_batch(df: pd.DataFrame, out_prefix: str):\n",
    "    # Generate KEY\n",
    "    if \"KEY\" not in df.columns:\n",
    "        df[\"KEY\"] = df.apply(\n",
    "            lambda r: row_key(\n",
    "                r.get(\"title\", \"\"),\n",
    "                r.get(\"authors\", \"\"),\n",
    "                r.get(\"year\", \"\"),\n",
    "                r.get(\"url\", \"\")\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # Resume: load completed keys\n",
    "    already = load_done_keys(out_prefix)\n",
    "\n",
    "    # Only process rows with empty abstract and not in completed keys\n",
    "    mask_no_abs = df[\"abstract\"].isna() | (df[\"abstract\"].astype(str).str.strip() == \"\")\n",
    "    todo = df[mask_no_abs & (~df[\"KEY\"].isin(already))].copy()\n",
    "\n",
    "    rows, seen = [], 0\n",
    "    pbar = tqdm(total=len(todo), desc=\"ðŸ”Ž Fetching abstracts (resumable)\")\n",
    "\n",
    "    for _, r in todo.iterrows():\n",
    "        title_guess = str(r.get(\"title\", \"\") or \"\").strip()\n",
    "        authors     = str(r.get(\"authors\", \"\") or \"\").strip()\n",
    "        year        = r.get(\"year\", \"\")\n",
    "\n",
    "        try:\n",
    "            res, score = fetch_one(title_guess, authors, year)\n",
    "        except Exception as e:\n",
    "            res, score = ({\"Abstract\": \"\", \"Source\": \"\", \"Status\": f\"error:{type(e).__name__}\"}, 0)\n",
    "\n",
    "        rec = {\n",
    "            \"KEY\": r.get(\"KEY\", \"\"),\n",
    "            \"title\": title_guess,\n",
    "            \"authors\": authors,\n",
    "            \"year\": year,\n",
    "            \"url\": r.get(\"url\", \"\"),\n",
    "            \"Abstract\": res.get(\"Abstract\", \"\"),\n",
    "            \"Source\": res.get(\"Source\", \"\"),\n",
    "            \"MatchScore\": score,\n",
    "            \"DOI\": res.get(\"DOI\", \"\"),\n",
    "            \"URL_found\": res.get(\"URL\", \"\"),\n",
    "            \"Year_Found\": res.get(\"Year_Found\", \"\"),\n",
    "            \"Venue\": res.get(\"Venue\", \"\"),\n",
    "            \"Status\": res.get(\"Status\", \"\"),\n",
    "            \"QueryUsed\": res.get(\"QueryUsed\", \"\"),\n",
    "            \"CandidateTitle\": res.get(\"CandidateTitle\", \"\"),\n",
    "            \"CandidateDOI\": res.get(\"CandidateDOI\", \"\"),\n",
    "            \"WhyNoAbstract\": res.get(\"WhyNoAbstract\", \"\"),\n",
    "        }\n",
    "        rows.append(rec)\n",
    "\n",
    "        seen += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Periodic autosave\n",
    "        if seen % AUTOSAVE_EVERY == 0:\n",
    "            tmp = pd.DataFrame(rows)\n",
    "            out_ok   = tmp[tmp[\"Abstract\"].astype(str).str.len() > 0]\n",
    "            out_todo = tmp[tmp[\"Abstract\"].astype(str).str.len() == 0]\n",
    "            if len(out_ok):\n",
    "                safe_to_excel(out_ok,   f\"{out_prefix}_autosave_with_abstracts.xlsx\")\n",
    "            if len(out_todo):\n",
    "                safe_to_excel(out_todo, f\"{out_prefix}_autosave_need_manual.xlsx\")\n",
    "\n",
    "        time.sleep(REQUEST_INTERVAL)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    cur = pd.DataFrame(rows)\n",
    "\n",
    "    def load_old(suf):\n",
    "        p = Path(f\"{out_prefix}{suf}\")\n",
    "        if p.exists():\n",
    "            try:\n",
    "                return pd.read_excel(p)\n",
    "            except Exception:\n",
    "                return pd.DataFrame()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    old_with  = load_old(\"_with_abstracts.xlsx\")\n",
    "    old_need  = load_old(\"_need_manual.xlsx\")\n",
    "    auto_with = load_old(\"_autosave_with_abstracts.xlsx\")\n",
    "    auto_need = load_old(\"_autosave_need_manual.xlsx\")\n",
    "\n",
    "    all_old = pd.concat([old_with, old_need, auto_with, auto_need], ignore_index=True, sort=False)\n",
    "\n",
    "    if len(all_old):\n",
    "        if \"KEY\" not in all_old.columns:\n",
    "            all_old[\"KEY\"] = all_old.apply(\n",
    "                lambda r: row_key(\n",
    "                    r.get(\"title\", \"\"),\n",
    "                    r.get(\"authors\", \"\"),\n",
    "                    r.get(\"year\", \"\"),\n",
    "                    r.get(\"url\", \"\")\n",
    "                ),\n",
    "                axis=1\n",
    "            )\n",
    "        merged = pd.concat([all_old, cur], ignore_index=True, sort=False)\n",
    "        merged = merged.drop_duplicates(subset=[\"KEY\"], keep=\"last\")\n",
    "    else:\n",
    "        merged = cur\n",
    "\n",
    "    out_ok   = merged[merged[\"Abstract\"].astype(str).str.len() > 0]\n",
    "    out_todo = merged[merged[\"Abstract\"].astype(str).str.len() == 0]\n",
    "\n",
    "    safe_to_excel(out_ok,   f\"{out_prefix}_with_abstracts.xlsx\")\n",
    "    safe_to_excel(out_todo, f\"{out_prefix}_need_manual.xlsx\")\n",
    "\n",
    "    print(f\"Done. With abstracts: {len(out_ok)} | Need manual: {len(out_todo)} | Total: {len(merged)}\")\n",
    "\n",
    "# ========== Entry ==========\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "\n",
    "    # Optional: filter by professor name\n",
    "    if PROFESSOR_NAME:\n",
    "        df = df[df[\"authors\"].astype(str).str.contains(PROFESSOR_NAME, case=False, na=False)].copy()\n",
    "        if df.empty:\n",
    "            print(f\"[âš ] Professor not found: {PROFESSOR_NAME}\")\n",
    "            sys.exit(0)\n",
    "        out_prefix = re.sub(r\"[^A-Za-z0-9]+\", \"_\", PROFESSOR_NAME).strip(\"_\")\n",
    "    else:\n",
    "        out_prefix = Path(INPUT_FILE).stem\n",
    "\n",
    "    # Generate KEY\n",
    "    if \"KEY\" not in df.columns:\n",
    "        df[\"KEY\"] = df.apply(\n",
    "            lambda r: row_key(\n",
    "                r.get(\"title\", \"\"),\n",
    "                r.get(\"authors\", \"\"),\n",
    "                r.get(\"year\", \"\"),\n",
    "                r.get(\"url\", \"\")\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    run_batch(df, out_prefix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
