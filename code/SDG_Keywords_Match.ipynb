{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac85fd-cce3-48db-994a-567ad9477373",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn sentence-transformers openpyxl  #if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a73227-abf1-46ef-a792-27b3d6bfe360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== Imports ===============\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# =============== 1. Config section (edit according to your setup) ===============\n",
    "\n",
    "# File paths\n",
    "ABSTRACT_FILE = \"YOUR FILE NAME.xlsx\"\n",
    "SDG_KEYWORD_FILE = \"SDG_keywords.xlsx\"\n",
    "\n",
    "# Column names (check against your own Excel)\n",
    "ABSTRACT_COL = \"abstract\"     # Column containing abstracts\n",
    "SDG_COL = \"goal\"              # SDG label, e.g. \"Goal 1\" ...\"Goal 17\"\n",
    "KEYWORDS_COL = \"keywords\"     # Comma-separated keyword phrases\n",
    "\n",
    "# Output column names\n",
    "PRIMARY_SDG_COL = \"Primary_SDG\"\n",
    "PRIMARY_SCORE_COL = \"Primary_SDG_Score\"\n",
    "MATCHED_KEYWORDS_COL = \"Primary_SDG_Semantic_Matched_Keywords\"\n",
    "\n",
    "# TF-IDF relevance threshold: used only to flag \"relevant or not\",\n",
    "# does NOT affect which SDG is chosen as Primary SDG\n",
    "RELEVANCE_THRESHOLD = 0.05   # You can tune this later, e.g. 0.05 or 0.10\n",
    "\n",
    "# Semantic keyword match threshold (higher = stricter)\n",
    "KEYWORD_SIM_THRESHOLD = 0.60   # Recommended start at 0.60; if too few hits, try 0.55\n",
    "\n",
    "# Output file name\n",
    "OUTPUT_FILE = \"YOUR_FILE_NAME.xlsx\"\n",
    "\n",
    "\n",
    "# =============== 2. Read data ===============\n",
    "\n",
    "df_abs = pd.read_excel(ABSTRACT_FILE)\n",
    "df_sdg = pd.read_excel(SDG_KEYWORD_FILE)\n",
    "\n",
    "# Fill missing abstracts with empty strings to avoid errors\n",
    "df_abs[ABSTRACT_COL] = df_abs[ABSTRACT_COL].fillna(\"\").astype(str)\n",
    "\n",
    "# Basic cleaning for SDG data\n",
    "df_sdg = df_sdg.dropna(subset=[SDG_COL, KEYWORDS_COL]).copy()\n",
    "df_sdg[SDG_COL] = df_sdg[SDG_COL].astype(str)\n",
    "df_sdg[KEYWORDS_COL] = df_sdg[KEYWORDS_COL].astype(str)\n",
    "\n",
    "\n",
    "# =============== 3. Build one \"keyword document\" per SDG (for TF-IDF) ===============\n",
    "\n",
    "# If one SDG appears on multiple rows, concatenate all its keywords into one long string\n",
    "# e.g. Goal 1 -> \"extreme poverty, income inequality, ... , social protection\"\n",
    "sdg_doc_series = df_sdg.groupby(SDG_COL)[KEYWORDS_COL].apply(\n",
    "    lambda s: \", \".join([str(x) for x in s if pd.notna(x)])\n",
    ")\n",
    "\n",
    "sdg_ids = list(sdg_doc_series.index)     # e.g. [\"Goal 1\", \"Goal 2\", ...]\n",
    "sdg_texts = list(sdg_doc_series.values) # The long keyword document for each SDG\n",
    "\n",
    "print(f\"Detected {len(sdg_ids)} SDGs: {sdg_ids}\")\n",
    "\n",
    "\n",
    "# =============== 4. TF-IDF: compute Primary SDG and Score ===============\n",
    "\n",
    "abstract_texts = df_abs[ABSTRACT_COL].tolist()\n",
    "\n",
    "# Corpus = SDG documents + all abstracts\n",
    "corpus = sdg_texts + abstract_texts\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),   # unigrams + bigrams\n",
    "    max_df=0.95,\n",
    "    min_df=1\n",
    ")\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "n_sdg = len(sdg_ids)\n",
    "sdg_matrix = tfidf_matrix[:n_sdg, :]      # SDG vectors\n",
    "abstract_matrix = tfidf_matrix[n_sdg:, :] # Abstract vectors\n",
    "\n",
    "# Cosine similarity: shape (num_abstracts, n_sdg)\n",
    "similarity_matrix = cosine_similarity(abstract_matrix, sdg_matrix)\n",
    "\n",
    "# For each abstract, find the SDG with the highest score\n",
    "primary_sdg_indices = similarity_matrix.argmax(axis=1)\n",
    "primary_scores = similarity_matrix[np.arange(similarity_matrix.shape[0]), primary_sdg_indices]\n",
    "primary_sdgs = [sdg_ids[idx] for idx in primary_sdg_indices]\n",
    "\n",
    "# Write back to DataFrame\n",
    "df_abs[PRIMARY_SDG_COL] = primary_sdgs\n",
    "df_abs[PRIMARY_SCORE_COL] = primary_scores\n",
    "\n",
    "# Optional: add a boolean flag \"Primary_SDG_Relevant\" based on the agreed threshold\n",
    "df_abs[\"Primary_SDG_Relevant\"] = df_abs[PRIMARY_SCORE_COL] >= RELEVANCE_THRESHOLD\n",
    "\n",
    "\n",
    "# =============== 5. Sentence-level semantic matching + fallback keywords ===============\n",
    "import re\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "print(\"Loading sentence embedding model...\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Semantic match threshold (controls how strict a \"normal\" keyword hit is)\n",
    "KEYWORD_SIM_THRESHOLD = 0.55  # You can tune this based on results\n",
    "\n",
    "# Fallback: only if Primary_SDG_Score >= this value, we force at least one keyword\n",
    "FALLBACK_PRIMARY_SCORE = 0.05\n",
    "\n",
    "# Simple sentence splitter\n",
    "def split_sentences(text: str):\n",
    "    text = str(text).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    parts = re.split(r'[。！？!?\\.]+', text)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "# 1) Prepare SDG -> [phrase list] and SDG -> phrase embeddings\n",
    "sdg_phrase_map = {}\n",
    "sdg_phrase_emb_map = {}\n",
    "\n",
    "for goal, big_str in sdg_doc_series.items():\n",
    "    phrases = [p.strip() for p in str(big_str).split(\",\") if p.strip()]\n",
    "    if not phrases:\n",
    "        continue\n",
    "    phrase_embs = model.encode(phrases, convert_to_tensor=True, show_progress_bar=False)\n",
    "    sdg_phrase_map[goal] = phrases\n",
    "    sdg_phrase_emb_map[goal] = phrase_embs\n",
    "\n",
    "print(f\"Computed keyword embeddings for {len(sdg_phrase_map)} SDGs.\")\n",
    "\n",
    "# Retrieve existing results\n",
    "abstract_texts = df_abs[ABSTRACT_COL].fillna(\"\").astype(str).tolist()\n",
    "primary_sdgs = df_abs[PRIMARY_SDG_COL].tolist()\n",
    "primary_scores_list = df_abs[PRIMARY_SCORE_COL].tolist()\n",
    "\n",
    "semantic_matched_keywords = []\n",
    "\n",
    "# 2) For each abstract, do sentence-level semantic matching + fallback\n",
    "for abs_text, sdg_id, primary_score in zip(abstract_texts, primary_sdgs, primary_scores_list):\n",
    "    abs_text = abs_text.strip()\n",
    "    if not abs_text or sdg_id not in sdg_phrase_map:\n",
    "        semantic_matched_keywords.append(\"\")\n",
    "        continue\n",
    "\n",
    "    sentences = split_sentences(abs_text)\n",
    "    if not sentences:\n",
    "        semantic_matched_keywords.append(\"\")\n",
    "        continue\n",
    "\n",
    "    # Encode all sentences at once\n",
    "    sent_embs = model.encode(sentences, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "    phrases = sdg_phrase_map[sdg_id]\n",
    "    phrase_embs = sdg_phrase_emb_map[sdg_id]   # [num_phrases, dim]\n",
    "\n",
    "    # sim_matrix: [num_phrases, num_sentences]\n",
    "    sim_matrix = util.cos_sim(phrase_embs, sent_embs)\n",
    "\n",
    "    matched = []\n",
    "    best_phrase = None\n",
    "    best_sim = -1.0\n",
    "\n",
    "    # For each phrase, take its maximum similarity over all sentences\n",
    "    for ph, sims in zip(phrases, sim_matrix):\n",
    "        max_sim = float(torch.max(sims))\n",
    "\n",
    "        # Track the single best phrase overall, for fallback\n",
    "        if max_sim > best_sim:\n",
    "            best_sim = max_sim\n",
    "            best_phrase = ph\n",
    "\n",
    "        # Normal hit: if above threshold, add to matched list\n",
    "        if max_sim >= KEYWORD_SIM_THRESHOLD:\n",
    "            matched.append(ph)\n",
    "\n",
    "    # If there are no normal matches but the primary_score is high enough,\n",
    "    # and we have a best_phrase, then use fallback to force at least one keyword.\n",
    "    if not matched and primary_score >= FALLBACK_PRIMARY_SCORE and best_phrase is not None:\n",
    "        # You could optionally tag this as a fallback, but here we keep it simple:\n",
    "        # matched = [f\"{best_phrase} (fallback, sim={best_sim:.2f})\"]\n",
    "        matched = [best_phrase]\n",
    "\n",
    "    matched = sorted(set(matched))\n",
    "    semantic_matched_keywords.append(\"; \".join(matched))\n",
    "\n",
    "# Write back to DataFrame\n",
    "df_abs[MATCHED_KEYWORDS_COL] = semantic_matched_keywords\n",
    "\n",
    "# =============== 6. Export results ===============\n",
    "\n",
    "df_abs.to_excel(OUTPUT_FILE, index=False)\n",
    "print(f\"All done! Results saved to: {OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
