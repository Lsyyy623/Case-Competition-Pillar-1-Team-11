{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6708e8-d144-46ae-bf73-d85fa6c1476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最终正确版 - 每篇publication作为独立行\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "import nest_asyncio\n",
    "import random\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def human_delay(min_sec=2, max_sec=4):\n",
    "    \"\"\"Random delay\"\"\"\n",
    "    await asyncio.sleep(random.uniform(min_sec, max_sec))\n",
    "\n",
    "async def scrape_professor_complete(page, prof_url, prof_name):\n",
    "    \"\"\"Visit professor page and extract all information, each publication as separate return\"\"\"\n",
    "    print(f\"  Visiting: {prof_name}\")\n",
    "    \n",
    "    # Basic information\n",
    "    basic_info = {\n",
    "        'name': prof_name,\n",
    "        'profile_url': prof_url,\n",
    "        'title': 'N/A',\n",
    "        'department': 'N/A'\n",
    "    }\n",
    "    \n",
    "    publications = []  # Store all publications\n",
    "    \n",
    "    try:\n",
    "        # Visit personal page\n",
    "        try:\n",
    "            await page.goto(prof_url, wait_until='load', timeout=120000)\n",
    "        except:\n",
    "            print(f\"    Load timeout, trying to continue...\")\n",
    "        \n",
    "        await asyncio.sleep(3)\n",
    "        \n",
    "        # Scroll to load content\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "                await asyncio.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # ===== Extract Title =====\n",
    "        try:\n",
    "            title_elem = await page.query_selector('.field--name-field-faculty-title .field__item')\n",
    "            if title_elem:\n",
    "                basic_info['title'] = (await title_elem.inner_text()).strip()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # ===== Extract Department =====\n",
    "        try:\n",
    "            # Method 1: Find via field--name-field-academic-area-full\n",
    "            dept_field = await page.query_selector('.field--name-field-academic-area-full .field__item')\n",
    "            if dept_field:\n",
    "                basic_info['department'] = (await dept_field.inner_text()).strip()\n",
    "            else:\n",
    "                # Method 2: Find \"Academic Areas\" label\n",
    "                dept_label = await page.query_selector('text=\"Academic Areas\"')\n",
    "                if dept_label:\n",
    "                    parent = await dept_label.evaluate_handle('el => el.closest(\".field\")')\n",
    "                    if parent:\n",
    "                        dept_item = await parent.query_selector('.field__item')\n",
    "                        if dept_item:\n",
    "                            basic_info['department'] = (await dept_item.inner_text()).strip()\n",
    "        except Exception as e:\n",
    "            print(f\"    Failed to extract Department: {str(e)}\")\n",
    "        \n",
    "        # ===== Extract Publications - Only from Articles tab =====\n",
    "        try:\n",
    "            # Method 1: Find directly from div with id=\"tabContentPaper\"\n",
    "            articles_tab_content = await page.query_selector('#tabContentPaper')\n",
    "            \n",
    "            pub_articles = []\n",
    "            if articles_tab_content:\n",
    "                # Find article tags only within this div\n",
    "                pub_articles = await articles_tab_content.query_selector_all('article.publication.publication--default')\n",
    "                print(f\"    Found {len(pub_articles)} articles in Articles tab\")\n",
    "            else:\n",
    "                # Backup: Click Articles tab then search\n",
    "                try:\n",
    "                    articles_button = await page.query_selector('#tabNavPaper')\n",
    "                    if articles_button:\n",
    "                        await articles_button.click()\n",
    "                        await asyncio.sleep(1)\n",
    "                        articles_tab_content = await page.query_selector('#tabContentPaper')\n",
    "                        if articles_tab_content:\n",
    "                            pub_articles = await articles_tab_content.query_selector_all('article.publication.publication--default')\n",
    "                            print(f\"    Found {len(pub_articles)} articles after clicking\")\n",
    "                except:\n",
    "                    print(f\"    Unable to locate Articles tab\")\n",
    "            \n",
    "            for idx, article in enumerate(pub_articles, 1):\n",
    "                try:\n",
    "                    pub_data = {}\n",
    "                    \n",
    "                    # Extract title\n",
    "                    title_elem = await article.query_selector('.publication__title')\n",
    "                    if title_elem:\n",
    "                        title_text = await title_elem.inner_text()\n",
    "                        pub_data['pub_title'] = title_text.strip()\n",
    "                    else:\n",
    "                        continue  # Skip if no title\n",
    "                    \n",
    "                    # Extract Published Date\n",
    "                    date_elem = await article.query_selector('.field--name-field-display-date .field__item')\n",
    "                    if date_elem:\n",
    "                        pub_data['published_date'] = (await date_elem.inner_text()).strip()\n",
    "                    else:\n",
    "                        pub_data['published_date'] = ''\n",
    "                    \n",
    "                    # Extract Authors\n",
    "                    authors_elem = await article.query_selector('.field--name-field-author-text .field__item')\n",
    "                    if authors_elem:\n",
    "                        pub_data['authors'] = (await authors_elem.inner_text()).strip()\n",
    "                    else:\n",
    "                        pub_data['authors'] = ''\n",
    "                    \n",
    "                    # Extract Source\n",
    "                    journal_elem = await article.query_selector('.field--name-field-journal-title')\n",
    "                    if journal_elem:\n",
    "                        pub_data['source'] = (await journal_elem.inner_text()).strip()\n",
    "                    else:\n",
    "                        pub_data['source'] = ''\n",
    "                    \n",
    "                    # Extract Volume\n",
    "                    volume_elem = await article.query_selector('.field--name-field-volume .field__item')\n",
    "                    if volume_elem:\n",
    "                        pub_data['volume'] = (await volume_elem.inner_text()).strip()\n",
    "                    else:\n",
    "                        pub_data['volume'] = ''\n",
    "                    \n",
    "                    # Extract Issue\n",
    "                    issue_elem = await article.query_selector('.field--name-field-issue .field__item')\n",
    "                    if issue_elem:\n",
    "                        pub_data['issue'] = (await issue_elem.inner_text()).strip()\n",
    "                    else:\n",
    "                        pub_data['issue'] = ''\n",
    "                    \n",
    "                    # Extract Pages\n",
    "                    pages_elem = await article.query_selector('.field--name-field-pages .field__item')\n",
    "                    if pages_elem:\n",
    "                        pub_data['pages'] = (await pages_elem.inner_text()).strip()\n",
    "                    else:\n",
    "                        pub_data['pages'] = ''\n",
    "                    \n",
    "                    publications.append(pub_data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Failed to extract publication #{idx}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"    Failed to extract Publications: {str(e)}\")\n",
    "        \n",
    "        print(f\"    Title: {basic_info['title']}\")\n",
    "        print(f\"    Department: {basic_info['department']}\")\n",
    "        print(f\"    Publications: {len(publications)} articles\")\n",
    "        \n",
    "        return basic_info, publications\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Error: {str(e)}\")\n",
    "        return basic_info, []\n",
    "\n",
    "async def scrape_all_faculty(csv_file='umich_ross_final.csv', start_from=0, end_at=None):\n",
    "    \"\"\"\n",
    "    Scrape complete information for all professors, each publication as a separate row\n",
    "    \n",
    "    DataFrame format:\n",
    "    | name | profile_url | title | department | pub_title | published_date | authors | source | volume | issue | pages |\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read existing data\n",
    "    try:\n",
    "        df_basic = pd.read_csv(csv_file)\n",
    "        print(f\"Read {len(df_basic)} professors' basic information\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {csv_file}\")\n",
    "        return None\n",
    "    \n",
    "    if end_at is None or end_at > len(df_basic):\n",
    "        end_at = len(df_basic)\n",
    "    \n",
    "    print(f\"Will process professors {start_from+1} to {end_at}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    all_rows = []  # Store all rows (one row per publication)\n",
    "    \n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)\n",
    "        \n",
    "        context = await browser.new_context(\n",
    "            viewport={'width': 1920, 'height': 1080},\n",
    "            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n",
    "        )\n",
    "        \n",
    "        await context.add_init_script(\"\"\"\n",
    "            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});\n",
    "        \"\"\")\n",
    "        \n",
    "        page = await context.new_page()\n",
    "        \n",
    "        try:\n",
    "            for idx in range(start_from, end_at):\n",
    "                row = df_basic.iloc[idx]\n",
    "                name = row['name']\n",
    "                url = row['profile_url']\n",
    "                \n",
    "                print(f\"[{idx+1}/{len(df_basic)}] {name}\")\n",
    "                \n",
    "                # Get detailed information\n",
    "                basic_info, publications = await scrape_professor_complete(page, url, name)\n",
    "                \n",
    "                # If has publications, create one row per publication\n",
    "                if publications:\n",
    "                    for pub in publications:\n",
    "                        combined_row = {\n",
    "                            'name': basic_info['name'],\n",
    "                            'profile_url': basic_info['profile_url'],\n",
    "                            'title': basic_info['title'],\n",
    "                            'department': basic_info['department'],\n",
    "                            'pub_title': pub.get('pub_title', ''),\n",
    "                            'published_date': pub.get('published_date', ''),\n",
    "                            'authors': pub.get('authors', ''),\n",
    "                            'source': pub.get('source', ''),\n",
    "                            'volume': pub.get('volume', ''),\n",
    "                            'issue': pub.get('issue', ''),\n",
    "                            'pages': pub.get('pages', '')\n",
    "                        }\n",
    "                        all_rows.append(combined_row)\n",
    "                else:\n",
    "                    # If no publications, create one row with empty publication fields\n",
    "                    combined_row = {\n",
    "                        'name': basic_info['name'],\n",
    "                        'profile_url': basic_info['profile_url'],\n",
    "                        'title': basic_info['title'],\n",
    "                        'department': basic_info['department'],\n",
    "                        'pub_title': 'No publications found',\n",
    "                        'published_date': '',\n",
    "                        'authors': '',\n",
    "                        'source': '',\n",
    "                        'volume': '',\n",
    "                        'issue': '',\n",
    "                        'pages': ''\n",
    "                    }\n",
    "                    all_rows.append(combined_row)\n",
    "                \n",
    "                # Save progress every 10 professors\n",
    "                if (idx + 1) % 10 == 0:\n",
    "                    df_temp = pd.DataFrame(all_rows)\n",
    "                    df_temp.to_csv('umich_ross_detailed_temp.csv', index=False, encoding='utf-8-sig')\n",
    "                    print(f\"\\nSaved progress: {len(all_rows)} rows\\n\")\n",
    "                \n",
    "                # Delay\n",
    "                await human_delay(3, 6)\n",
    "            \n",
    "            # Create final DataFrame\n",
    "            df_final = pd.DataFrame(all_rows)\n",
    "            \n",
    "            # Save\n",
    "            df_final.to_csv('umich_ross_detailed_final.csv', index=False, encoding='utf-8-sig')\n",
    "            try:\n",
    "                df_final.to_excel('umich_ross_detailed_final.xlsx', index=False, engine='openpyxl')\n",
    "            except:\n",
    "                print(\"Unable to save Excel file\")\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Complete!\")\n",
    "            print(f\"Total professors: {len(df_basic[start_from:end_at])}\")\n",
    "            print(f\"Total rows: {len(all_rows)} (one row per publication)\")\n",
    "            print(f\"Data saved to:\")\n",
    "            print(f\"   - umich_ross_detailed_final.csv\")\n",
    "            print(f\"   - umich_ross_detailed_final.xlsx\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "            \n",
    "            return df_final\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nUser interrupted, saving current progress...\")\n",
    "            if all_rows:\n",
    "                df = pd.DataFrame(all_rows)\n",
    "                df.to_csv('umich_ross_detailed_interrupted.csv', index=False, encoding='utf-8-sig')\n",
    "            return pd.DataFrame(all_rows) if all_rows else None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "            if all_rows:\n",
    "                df = pd.DataFrame(all_rows)\n",
    "                df.to_csv('umich_ross_detailed_error.csv', index=False, encoding='utf-8-sig')\n",
    "            return pd.DataFrame(all_rows) if all_rows else None\n",
    "            \n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "# =============================================================================\n",
    "# Complete Usage Guide\n",
    "# =============================================================================\n",
    "\n",
    "# [Step 1] Ensure you have CSV file with all professors' names and URLs\n",
    "# If not, run this first (in another cell):\n",
    "# df_basic = await run_scraper(total_pages=9, fetch_publications=False)\n",
    "\n",
    "# [Step 2] Test with first 3 professors to ensure everything works\n",
    "# df_test = await scrape_all_faculty('umich_ross_final.csv', start_from=0, end_at=3)\n",
    "# display(df_test)\n",
    "\n",
    "# [Step 3] Scrape all! Expected 1-2 hours, auto-saves progress\n",
    "df_all = await scrape_all_faculty('umich_ross_final.csv')\n",
    "\n",
    "# [Step 4] View final results\n",
    "if df_all is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Success!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total rows: {len(df_all)}\")\n",
    "    print(f\"Total professors: {df_all['name'].nunique()}\")\n",
    "    print(f\"Total publications: {len(df_all[df_all['pub_title'] != 'No publications found'])}\")\n",
    "    print(\"\\nData preview:\")\n",
    "    display(df_all.head(20))\n",
    "    \n",
    "    # Statistics by professor\n",
    "    pub_counts = df_all[df_all['pub_title'] != 'No publications found'].groupby('name').size()\n",
    "    print(f\"\\nPublications statistics:\")\n",
    "    print(f\"  Average per professor: {pub_counts.mean():.1f} articles\")\n",
    "    print(f\"  Maximum: {pub_counts.max()} articles\")\n",
    "    print(f\"  Minimum: {pub_counts.min()} articles\")\n",
    "\n",
    "# =============================================================================\n",
    "# Advanced Usage\n",
    "# =============================================================================\n",
    "\n",
    "# If interrupted, continue from position 50:\n",
    "# df_resume = await scrape_all_faculty('umich_ross_final.csv', start_from=50)\n",
    "\n",
    "# Scrape specific range (e.g., positions 20-30):\n",
    "# df_range = await scrape_all_faculty('umich_ross_final.csv', start_from=20, end_at=30)\n",
    "\n",
    "# View all publications for a specific professor:\n",
    "# prof_name = \"Allan Afuah\"  # Replace with desired professor\n",
    "# prof_pubs = df_all[df_all['name'] == prof_name]\n",
    "# display(prof_pubs)\n",
    "\n",
    "# Export data for specific department:\n",
    "# strategy_profs = df_all[df_all['department'] == 'Strategy']\n",
    "# strategy_profs.to_csv('strategy_faculty.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8482f59-751a-4ba7-884d-42993e2deef8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           name                                        profile_url  \\\n",
      "0   Allan Afuah  https://michiganross.umich.edu/faculty-researc...   \n",
      "1   Allan Afuah  https://michiganross.umich.edu/faculty-researc...   \n",
      "2   Allan Afuah  https://michiganross.umich.edu/faculty-researc...   \n",
      "3   Allan Afuah  https://michiganross.umich.edu/faculty-researc...   \n",
      "4   Allan Afuah  https://michiganross.umich.edu/faculty-researc...   \n",
      "5   Allan Afuah  https://michiganross.umich.edu/faculty-researc...   \n",
      "6   Allan Afuah  https://michiganross.umich.edu/faculty-researc...   \n",
      "7   Allan Afuah  https://michiganross.umich.edu/faculty-researc...   \n",
      "8  Hyun-Soo Ahn  https://michiganross.umich.edu/faculty-researc...   \n",
      "9  Hyun-Soo Ahn  https://michiganross.umich.edu/faculty-researc...   \n",
      "\n",
      "                                    title                 department  \\\n",
      "0                   Professor of Strategy                   Strategy   \n",
      "1                   Professor of Strategy                   Strategy   \n",
      "2                   Professor of Strategy                   Strategy   \n",
      "3                   Professor of Strategy                   Strategy   \n",
      "4                   Professor of Strategy                   Strategy   \n",
      "5                   Professor of Strategy                   Strategy   \n",
      "6                   Professor of Strategy                   Strategy   \n",
      "7                   Professor of Strategy                   Strategy   \n",
      "8  Professor of Technology and Operations  Technology and Operations   \n",
      "9  Professor of Technology and Operations  Technology and Operations   \n",
      "\n",
      "                                           pub_title published_date  \\\n",
      "0  Developing a Theory of the Firm for the 21st C...        10/2020   \n",
      "1  Editors’ Comments: Should management theories ...        04/2018   \n",
      "2  A Critical Assessment of Business Model Resear...        01/2017   \n",
      "3   Value capture and crowdsourcing (dialogue piece)         7/2014   \n",
      "4  Are network effects really all about size? The...        03/2013   \n",
      "5      Crowdsourcing as a solution to distant search        07/2012   \n",
      "6  Profiting from innovations: the role of new ga...         3/2010   \n",
      "7  Users as innovators: A review and future resea...         7/2010   \n",
      "8  Certainty-Equivalent Pricing with Dependent De...        02/2025   \n",
      "9  Optimal Subsidy Policy for Innovation: Technol...        05/2024   \n",
      "\n",
      "                                             authors  \\\n",
      "0  Alvarez, S. A., Zander, U., Barney, J. B., & A...   \n",
      "1               Alvarez, S., Afuah, A., & Gibson, C.   \n",
      "2                 Massa, L., Tucci, CL., & Afuah, A.   \n",
      "3                  Allan Afuah and Christopher Tucci   \n",
      "4                                        Allan Afuah   \n",
      "5                  Allan Afuah and Christopher Tucci   \n",
      "6                                        Allan Afuah   \n",
      "7                                        Allan Afuah   \n",
      "8  Hyun-soo Ahn, Chris Ryan, Joline Uichanco, Zhe...   \n",
      "9  Myunghoon Lee, Hyun-soo Ahn, Hak-jin Chung, Sa...   \n",
      "\n",
      "                                 source volume issue    pages  \n",
      "0          Academy of Management Review     45     4        5  \n",
      "1          Academy of Management Review     43     2    69-72  \n",
      "2          Academy of Management Annals     17     1   73-104  \n",
      "3          Academy of Management Review     38     3  457-460  \n",
      "4          Strategic Management Journal     34     3  257-273  \n",
      "5          Academy of Management Review     37     3  355-375  \n",
      "6                        R&D Management     40     2  124-137  \n",
      "7                 Journal of Management     36     4  857-875  \n",
      "8    Mathematics of Operations Research    NaN   NaN      NaN  \n",
      "9  Production and Operations Management    NaN   NaN      NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取你的解析后的 Excel\n",
    "df = pd.read_excel(\"umich_ross_detailed_final.xlsx\")\n",
    "\n",
    "# 打印前 10 行\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d207ab2-53c9-42ea-8c25-a01b6e166f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
