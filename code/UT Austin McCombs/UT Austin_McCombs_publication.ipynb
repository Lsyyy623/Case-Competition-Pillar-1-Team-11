{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6917455f-d0f3-4d43-86e8-089647db4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 2: Scrape Publications\n",
    "Reads faculty list from Cell 1 and scrapes publications for each faculty\n",
    "\"\"\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Setup Chrome browser\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "    chrome_options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "    chrome_options.add_argument('--disable-extensions')\n",
    "    chrome_options.add_argument('--disable-infobars')\n",
    "    chrome_options.add_argument('--remote-debugging-port=9222')\n",
    "    \n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Method 1 failed: {e}\")\n",
    "        try:\n",
    "            driver = webdriver.Chrome(options=chrome_options)\n",
    "            return driver\n",
    "        except Exception as e2:\n",
    "            print(f\"Method 2 also failed: {e2}\")\n",
    "            raise\n",
    "\n",
    "def scrape_publications(driver, profile_url):\n",
    "    \"\"\"Scrape publications from faculty profile page\"\"\"\n",
    "    publications = []\n",
    "    \n",
    "    try:\n",
    "        driver.get(profile_url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Click Publications tab - target the specific section\n",
    "        try:\n",
    "            # Find the Publications section link by data-hash\n",
    "            pub_tab_selector = \"//section[@data-hash='#Publications']//ancestor::div//a[contains(text(), 'Publications')] | //a[@href='#Publications'] | //*[@data-hash='#Publications']//preceding::a[contains(text(), 'Publications')][1]\"\n",
    "            \n",
    "            try:\n",
    "                pub_tab = driver.find_element(By.XPATH, pub_tab_selector)\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});\", pub_tab)\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "                try:\n",
    "                    pub_tab.click()\n",
    "                except:\n",
    "                    driver.execute_script(\"arguments[0].click();\", pub_tab)\n",
    "                \n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                # Fallback: try generic Publications link\n",
    "                pub_tab = driver.find_element(By.XPATH, \"//a[contains(text(), 'Publications')]\")\n",
    "                driver.execute_script(\"arguments[0].click();\", pub_tab)\n",
    "                time.sleep(2)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # CRITICAL: Only get publications from the Publications section\n",
    "        # Use very specific selector to avoid other sections\n",
    "        pub_selector = \"//section[@data-hash='#Publications']//li[@class='dm-profile-activity']\"\n",
    "        \n",
    "        try:\n",
    "            pub_elements = driver.find_elements(By.XPATH, pub_selector)\n",
    "            \n",
    "            # Only get visible elements\n",
    "            for pub in pub_elements:\n",
    "                if pub.is_displayed():\n",
    "                    text = pub.text.strip()\n",
    "                    # Additional filtering: must be substantial text\n",
    "                    if text and len(text) > 30:\n",
    "                        # Exclude if it looks like a section header or navigation\n",
    "                        if not any(x in text for x in ['About', 'Education', 'Awards', 'Honors', 'Publications', 'tabindex']):\n",
    "                            publications.append(text)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        publications = list(dict.fromkeys(publications))\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return publications\n",
    "\n",
    "# Main execution for Cell 2\n",
    "print(\"=\" * 70)\n",
    "print(\"McCombs Faculty Scraper - Part 2: Publications\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load faculty list from Cell 1\n",
    "try:\n",
    "    df_faculty = pd.read_csv('faculty_list.csv')\n",
    "    print(f\"\\n✅ Loaded {len(df_faculty)} faculty members from faculty_list.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n❌ Error: faculty_list.csv not found!\")\n",
    "    print(\"Please run Cell 1 first to generate the faculty list.\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nInitializing browser...\")\n",
    "driver = setup_driver()\n",
    "\n",
    "try:\n",
    "    print(\"\\nStep 2: Scraping publications...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    all_publications = []\n",
    "    \n",
    "    for i, row in enumerate(tqdm(df_faculty.iterrows(), total=len(df_faculty), desc=\"Scraping publications\"), 1):\n",
    "        faculty = row[1]\n",
    "        \n",
    "        try:\n",
    "            publications = scrape_publications(driver, faculty['profile_url'])\n",
    "            \n",
    "            # Create one record per publication\n",
    "            if publications:\n",
    "                for pub in publications:\n",
    "                    all_publications.append({\n",
    "                        'Name': faculty['name'],\n",
    "                        'Title': faculty['title'],\n",
    "                        'Department': faculty['department'],\n",
    "                        'Profile URL': faculty['profile_url'],\n",
    "                        'Publication': pub\n",
    "                    })\n",
    "            else:\n",
    "                # Keep one record even if no publications\n",
    "                all_publications.append({\n",
    "                    'Name': faculty['name'],\n",
    "                    'Title': faculty['title'],\n",
    "                    'Department': faculty['department'],\n",
    "                    'Profile URL': faculty['profile_url'],\n",
    "                    'Publication': \"\"\n",
    "                })\n",
    "            \n",
    "            # Progress update every 10 faculty\n",
    "            if i % 10 == 0:\n",
    "                print(f\"\\n  Processed {i}/{len(df_faculty)} faculty, {len(all_publications)} total records\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  Error processing {faculty['name']}: {e}\")\n",
    "            all_publications.append({\n",
    "                'Name': faculty['name'],\n",
    "                'Title': faculty['title'],\n",
    "                'Department': faculty['department'],\n",
    "                'Profile URL': faculty['profile_url'],\n",
    "                'Publication': \"\"\n",
    "            })\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_publications)\n",
    "    \n",
    "    # Display statistics\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Data Statistics:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total records (rows): {len(df)}\")\n",
    "    print(f\"Total faculty: {df['Name'].nunique()}\")\n",
    "    print(f\"Total publications: {df['Publication'].astype(bool).sum()}\")\n",
    "    print(f\"Faculty with publications: {df[df['Publication'] != '']['Name'].nunique()}\")\n",
    "    \n",
    "    # Preview\n",
    "    print(\"\\nFirst 10 records preview:\")\n",
    "    print(df[['Name', 'Title', 'Publication']].head(10).to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = 'mccombs_faculty_publications.csv'\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n✅ Data saved to: {output_file}\")\n",
    "    \n",
    "    # Top 5 faculty by publication count\n",
    "    print(\"\\nTop 5 faculty by publication count:\")\n",
    "    pub_counts = df[df['Publication'] != ''].groupby('Name').size().sort_values(ascending=False).head()\n",
    "    for name, count in pub_counts.items():\n",
    "        print(f\"  {name}: {count} publications\")\n",
    "    \n",
    "finally:\n",
    "    print(\"\\nClosing browser...\")\n",
    "    driver.quit()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Cell 2 Complete! Publications scraped successfully.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08492850-3c1d-4cbb-9ef1-30a2d9fe1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove Empty Publications\n",
    "Remove rows where Publication field is empty or whitespace only\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('mccombs_faculty_publications.csv')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Remove Empty Publications\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nOriginal data:\")\n",
    "print(f\"  Total rows: {len(df)}\")\n",
    "print(f\"  Total faculty: {df['Name'].nunique()}\")\n",
    "\n",
    "# Count empty publications\n",
    "empty_count = (df['Publication'].isna() | (df['Publication'].str.strip() == '')).sum()\n",
    "print(f\"  Empty publications: {empty_count}\")\n",
    "\n",
    "# Remove empty publications\n",
    "df_cleaned = df[df['Publication'].notna() & (df['Publication'].str.strip() != '')].copy()\n",
    "\n",
    "print(f\"\\nAfter removing empty publications:\")\n",
    "print(f\"  Total rows: {len(df_cleaned)}\")\n",
    "print(f\"  Total faculty: {df_cleaned['Name'].nunique()}\")\n",
    "print(f\"  Removed: {empty_count} rows\")\n",
    "\n",
    "# Preview\n",
    "print(f\"\\nFirst 5 rows preview:\")\n",
    "print(df_cleaned[['Name', 'Publication']].head().to_string(index=False))\n",
    "\n",
    "# Save cleaned data\n",
    "output_file = 'mccombs_faculty_publications_cleaned.csv'\n",
    "df_cleaned.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n✅ Cleaned data saved to: {output_file}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae47cf4-3f4c-49d0-8811-d46e8874d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract Title from Publication\n",
    "Add a 'title' column extracted from Publication field\n",
    "Note: Name and Year already exist in the dataset\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the cleaned data\n",
    "df = pd.read_csv('mccombs_faculty_publications_cleaned.csv')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Extract Title from Publication\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nInput data:\")\n",
    "print(f\"  Total rows: {len(df)}\")\n",
    "print(f\"  Existing columns: {', '.join(df.columns)}\")\n",
    "\n",
    "def extract_title(text):\n",
    "    \"\"\"\n",
    "    Extract paper title from publication text\n",
    "    \n",
    "    Title is usually:\n",
    "    1. In double quotes \"Title\"\n",
    "    2. Between year and period/journal name\n",
    "    3. After author names and year\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # Method 1: Extract text in double quotes \"Title\"\n",
    "    match = re.search(r'\"([^\"]+)\"', text)\n",
    "    if match:\n",
    "        title = match.group(1).strip()\n",
    "        return title\n",
    "    \n",
    "    # Method 2: Extract after year, before period or <i> tag\n",
    "    # Pattern: ... 2024. Title. <i>Journal</i> or ... 2024. Title?. Journal\n",
    "    year_match = re.search(r'\\b(19|20)\\d{2}\\b', text)\n",
    "    if year_match:\n",
    "        # Get text after year\n",
    "        after_year = text[year_match.end():].strip()\n",
    "        \n",
    "        # Remove leading punctuation\n",
    "        after_year = after_year.lstrip('., ')\n",
    "        \n",
    "        # Extract until: period, <i>, or end\n",
    "        # Try to find end markers\n",
    "        end_markers = [\n",
    "            r'\\.\\s*<i>',           # Period before italic (journal name)\n",
    "            r'\\?\\.',               # Question mark + period\n",
    "            r'\\.\\s*[A-Z][a-z]',   # Period before capitalized word (might be journal)\n",
    "            r'\\.\\s*\\d',            # Period before number (volume/issue)\n",
    "        ]\n",
    "        \n",
    "        for marker in end_markers:\n",
    "            marker_match = re.search(marker, after_year)\n",
    "            if marker_match:\n",
    "                title = after_year[:marker_match.start()].strip()\n",
    "                # Clean up trailing punctuation\n",
    "                title = title.rstrip('.,?! ')\n",
    "                if len(title) > 10:  # Reasonable title length\n",
    "                    return title\n",
    "        \n",
    "        # If no marker found, take first sentence\n",
    "        sentence_match = re.match(r'([^.?!]+)', after_year)\n",
    "        if sentence_match:\n",
    "            title = sentence_match.group(1).strip()\n",
    "            title = title.rstrip('.,?! ')\n",
    "            if len(title) > 10:\n",
    "                return title\n",
    "    \n",
    "    # Method 3: If still no title, return first 100 chars as fallback\n",
    "    return text[:100].strip()\n",
    "\n",
    "# Extract title\n",
    "print(\"\\nExtracting titles...\")\n",
    "df['title'] = df['Publication'].apply(extract_title)\n",
    "\n",
    "# Statistics\n",
    "successful_extractions = (df['title'].str.len() > 10).sum()\n",
    "print(f\"\\n✅ Successfully extracted {successful_extractions} titles ({successful_extractions/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for potential issues\n",
    "short_titles = df[df['title'].str.len() < 10]\n",
    "if len(short_titles) > 0:\n",
    "    print(f\"\\n⚠️ Warning: {len(short_titles)} titles are very short (<10 chars)\")\n",
    "    print(\"First 3 examples:\")\n",
    "    for i in range(min(3, len(short_titles))):\n",
    "        print(f\"  {i+1}. Title: '{short_titles.iloc[i]['title']}'\")\n",
    "        print(f\"     Original: {short_titles.iloc[i]['Publication'][:100]}...\")\n",
    "\n",
    "# Display examples\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Examples of extracted titles:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(min(5, len(df))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Title: {df.iloc[i]['title']}\")\n",
    "    print(f\"  Original: {df.iloc[i]['Publication'][:150]}...\")\n",
    "\n",
    "# Reorder columns to put title near the front\n",
    "# Assuming typical columns: Name, Title (job title), Department, Profile URL, Publication, Year\n",
    "if 'Year' in df.columns:\n",
    "    # If Year already exists, put title after it\n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove('title')\n",
    "    \n",
    "    # Find where to insert title (after Year if it exists)\n",
    "    if 'Year' in cols:\n",
    "        year_idx = cols.index('Year')\n",
    "        cols.insert(year_idx + 1, 'title')\n",
    "    else:\n",
    "        # Put title as second column\n",
    "        cols.insert(1, 'title')\n",
    "    \n",
    "    df = df[cols]\n",
    "else:\n",
    "    # If no Year column, just keep title at the end\n",
    "    pass\n",
    "\n",
    "# Preview final structure\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Final data structure:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Columns: {', '.join(df.columns)}\")\n",
    "print(f\"\\nFirst 5 rows preview:\")\n",
    "preview_cols = ['Name', 'title'] + ([col for col in df.columns if col not in ['Name', 'title', 'Publication']])\n",
    "if len(preview_cols) > 5:\n",
    "    preview_cols = preview_cols[:5]\n",
    "print(df[preview_cols].head().to_string(index=False))\n",
    "\n",
    "# Save with title column\n",
    "output_file = 'mccombs_faculty_publications_with_title.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n✅ Data with title column saved to: {output_file}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Complete! You now have:\")\n",
    "print(\"- Name (already existed)\")\n",
    "print(\"- title (newly extracted) ⭐\")\n",
    "print(\"- Year (already existed)\")\n",
    "print(\"- Publication (original)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce87b8dc-0c9c-4908-852c-ec74e579d4be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Name                                              Title  \\\n",
      "0  Christopher Aarons                                  The Digital Helix   \n",
      "1  Christopher Aarons  Digital Transformation: Delivering on the Promise   \n",
      "2  Christopher Aarons  Aarons, C. “Why People Matter Far More Than Di...   \n",
      "3      Ashish Agarwal  Follow Your Heart or Listen to Users? The Case...   \n",
      "4      Ashish Agarwal  Demand-Side Effects of Open Innovation: The Ca...   \n",
      "5      Ashish Agarwal  The Effect of Popularity Cues and Peer Endorse...   \n",
      "6      Ashish Agarwal  Liu, Y., Agarwal, A., Lai, G. and Zhou, W. On-...   \n",
      "7      Ashish Agarwal  Yang, C., Agarwal, A. and Konana, P. General B...   \n",
      "8      Ashish Agarwal  Artificial Intelligence, Collusion and Ad Auct...   \n",
      "9      Ashish Agarwal  Promotional Inventory Displays: An Empirical A...   \n",
      "\n",
      "                                     Department    Year  \\\n",
      "0                                     Marketing     NaN   \n",
      "1                                     Marketing     NaN   \n",
      "2                                     Marketing     NaN   \n",
      "3  Information, Risk, and Operations Management  2025.0   \n",
      "4  Information, Risk, and Operations Management  2025.0   \n",
      "5  Information, Risk, and Operations Management  2025.0   \n",
      "6  Information, Risk, and Operations Management     NaN   \n",
      "7  Information, Risk, and Operations Management     NaN   \n",
      "8  Information, Risk, and Operations Management  2024.0   \n",
      "9  Information, Risk, and Operations Management  2024.0   \n",
      "\n",
      "                                               title  \\\n",
      "0                                  The Digital Helix   \n",
      "1  Digital Transformation: Delivering on the Promise   \n",
      "2  Aarons, C. “Why People Matter Far More Than Di...   \n",
      "3  Follow Your Heart or Listen to Users? The Case...   \n",
      "4  Demand-Side Effects of Open Innovation: The Ca...   \n",
      "5  The Effect of Popularity Cues and Peer Endorse...   \n",
      "6  Liu, Y., Agarwal, A., Lai, G. and Zhou, W. On-...   \n",
      "7  Yang, C., Agarwal, A. and Konana, P. General B...   \n",
      "8  Artificial Intelligence, Collusion and Ad Auct...   \n",
      "9  Promotional Inventory Displays: An Empirical A...   \n",
      "\n",
      "                                         Profile URL  \\\n",
      "0  https://www.mccombs.utexas.edu/faculty-and-res...   \n",
      "1  https://www.mccombs.utexas.edu/faculty-and-res...   \n",
      "2  https://www.mccombs.utexas.edu/faculty-and-res...   \n",
      "3  https://www.mccombs.utexas.edu/faculty-and-res...   \n",
      "4  https://www.mccombs.utexas.edu/faculty-and-res...   \n",
      "5  https://www.mccombs.utexas.edu/faculty-and-res...   \n",
      "6  https://www.mccombs.utexas.edu/faculty-and-res...   \n",
      "7  https://www.mccombs.utexas.edu/faculty-and-res...   \n",
      "8  https://www.mccombs.utexas.edu/faculty-and-res...   \n",
      "9  https://www.mccombs.utexas.edu/faculty-and-res...   \n",
      "\n",
      "                                                 DOI  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3  https://doi.org/https://doi.org/10.1287/isre.2...   \n",
      "4  https://doi.org/https://doi.org/10.1287/mnsc.2...   \n",
      "5  https://doi.org/https://doi.org/10.1287/isre.2...   \n",
      "6                                                NaN   \n",
      "7                                                NaN   \n",
      "8                                                NaN   \n",
      "9  https://doi.org/https://doi.org/10.1287/msom.2...   \n",
      "\n",
      "                                         Publication  \n",
      "0         Aarons, C. \"The Digital Helix\". Greenleaf.  \n",
      "1  Aarons, C. \"Digital Transformation: Delivering...  \n",
      "2  Aarons, C. “Why People Matter Far More Than Di...  \n",
      "3  Karanam, S.A., Agarwal, A. and Barua, A. 2025....  \n",
      "4  Sharma, V., Agarwal, A. and Barua, A. 2025. De...  \n",
      "5  Agarwal, A., Lee, S. and Whinston, A.B. 2025. ...  \n",
      "6  Liu, Y., Agarwal, A., Lai, G. and Zhou, W. On-...  \n",
      "7  Yang, C., Agarwal, A. and Konana, P. General B...  \n",
      "8  Karanam, A., Tan, D., Agarwal, A. and Barua, A...  \n",
      "9  Zheng, J., Agarwal, A. and Stamatapoulos, I. 2...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"mccombs_faculty_publications_with_title.csv\")\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b665c21-fb8d-4d53-a565-fde0941d29be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
