{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9fa03f-59fb-4a52-9586-956d523e2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Wisconsin Business School Faculty List Scraper (Revised Version)\n",
    "# Handles pagination and filters\n",
    "# ===============================================================\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "print(\"üöÄ Wisconsin Business School Faculty List Scraper (Revised Version)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configure browser\n",
    "options = webdriver.ChromeOptions()\n",
    "# Uncomment to enable headless mode\n",
    "# options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()), \n",
    "    options=options\n",
    ")\n",
    "\n",
    "try:\n",
    "    # =============== Step 1: Visit page and set filter ===============\n",
    "    print(\"\\nüìå Step 1: Visiting Directory page...\")\n",
    "    \n",
    "    directory_url = \"https://business.wisc.edu/directory/\"\n",
    "    driver.get(directory_url)\n",
    "    \n",
    "    print(\"‚è≥ Waiting for page to load...\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Find and select \"Faculty\" filter\n",
    "    print(\"\\nüîç Looking for 'Type' filter...\")\n",
    "    \n",
    "    try:\n",
    "        # Find dropdown with class='facetwp-dropdown'\n",
    "        type_select = None\n",
    "        \n",
    "        selects = driver.find_elements(By.CLASS_NAME, \"facetwp-dropdown\")\n",
    "        \n",
    "        if selects:\n",
    "            # Usually the first one is the Type filter\n",
    "            type_select = selects[0]\n",
    "            print(f\"‚úì Found filter (class='facetwp-dropdown')\")\n",
    "            \n",
    "            select_obj = Select(type_select)\n",
    "            \n",
    "            # Show all options\n",
    "            print(\"\\nAvailable options:\")\n",
    "            for idx, option in enumerate(select_obj.options, 1):\n",
    "                print(f\"  {idx}. {option.text}\")\n",
    "            \n",
    "            # Select \"Faculty\"\n",
    "            try:\n",
    "                select_obj.select_by_value(\"faculty\")\n",
    "                print(\"\\n‚úÖ Selected 'Faculty' (220 members)\")\n",
    "                time.sleep(5)  # Wait for page update and load\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ö†Ô∏è  Selection failed: {e}\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Filter not found\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Filter operation failed: {e}\")\n",
    "        print(\"Continuing to scrape current displayed content...\")\n",
    "    \n",
    "    # =============== Step 2: Get all Faculty from all pages ===============\n",
    "    print(\"\\nüìå Step 2: Traversing all pages...\")\n",
    "    \n",
    "    all_profile_urls = set()\n",
    "    current_page = 1\n",
    "    max_pages = 20  # Try maximum 20 pages (you said there are 19 pages)\n",
    "    \n",
    "    while current_page <= max_pages:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üìÑ Page {current_page}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Wait for page to load\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Scroll page to ensure all content is visible\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Get all profile links on current page\n",
    "        page_links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        page_profile_count = 0\n",
    "        \n",
    "        for link in page_links:\n",
    "            try:\n",
    "                href = link.get_attribute('href')\n",
    "                if href and '/profile/' in href and 'business.wisc.edu' in href:\n",
    "                    if href not in all_profile_urls:\n",
    "                        all_profile_urls.add(href)\n",
    "                        page_profile_count += 1\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(f\"  ‚úì Added {page_profile_count} Faculty on this page\")\n",
    "        print(f\"  ‚úì Total collected {len(all_profile_urls)} Faculty\")\n",
    "        \n",
    "        # Find next page button\n",
    "        next_clicked = False\n",
    "        \n",
    "        try:\n",
    "            next_page_num = current_page + 1\n",
    "            \n",
    "            # Scroll to bottom of page to ensure pagination buttons are visible\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Method 1: Use JavaScript to find and click FacetWP pagination\n",
    "            try:\n",
    "                # FacetWP uses specific pagination structure\n",
    "                js_click = f\"\"\"\n",
    "                var pages = document.querySelectorAll('.facetwp-page');\n",
    "                for (var i = 0; i < pages.length; i++) {{\n",
    "                    if (pages[i].textContent.trim() === '{next_page_num}') {{\n",
    "                        pages[i].click();\n",
    "                        return true;\n",
    "                    }}\n",
    "                }}\n",
    "                return false;\n",
    "                \"\"\"\n",
    "                result = driver.execute_script(js_click)\n",
    "                if result:\n",
    "                    print(f\"  üîò Clicked page {next_page_num} (JavaScript)\")\n",
    "                    next_clicked = True\n",
    "                    time.sleep(4)\n",
    "            except Exception as e:\n",
    "                print(f\"  Method 1 failed: {e}\")\n",
    "            \n",
    "            # Method 2: Find button containing number and click with JavaScript\n",
    "            if not next_clicked:\n",
    "                try:\n",
    "                    page_buttons = driver.find_elements(By.XPATH, \n",
    "                        f\"//a[contains(@class, 'facetwp-page') and text()='{next_page_num}']\")\n",
    "                    \n",
    "                    if page_buttons:\n",
    "                        # Scroll to button position\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", page_buttons[0])\n",
    "                        time.sleep(0.5)\n",
    "                        # Click using JavaScript\n",
    "                        driver.execute_script(\"arguments[0].click();\", page_buttons[0])\n",
    "                        print(f\"  üîò Clicked page {next_page_num}\")\n",
    "                        next_clicked = True\n",
    "                        time.sleep(4)\n",
    "                except Exception as e:\n",
    "                    print(f\"  Method 2 failed: {e}\")\n",
    "            \n",
    "            # Method 3: Find \"Next\" button\n",
    "            if not next_clicked:\n",
    "                try:\n",
    "                    next_buttons = driver.find_elements(By.XPATH, \n",
    "                        \"//a[contains(@class, 'facetwp-page') and (contains(text(), '‚Ä∫') or contains(text(), 'Next'))]\")\n",
    "                    \n",
    "                    if next_buttons:\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_buttons[0])\n",
    "                        time.sleep(0.5)\n",
    "                        driver.execute_script(\"arguments[0].click();\", next_buttons[0])\n",
    "                        print(f\"  üîò Clicked 'Next' button\")\n",
    "                        next_clicked = True\n",
    "                        time.sleep(4)\n",
    "                except Exception as e:\n",
    "                    print(f\"  Method 3 failed: {e}\")\n",
    "            \n",
    "            # Method 4: Find all pagination links\n",
    "            if not next_clicked:\n",
    "                try:\n",
    "                    all_page_links = driver.find_elements(By.CSS_SELECTOR, \"a.facetwp-page, .pagination a\")\n",
    "                    print(f\"  Found {len(all_page_links)} pagination links\")\n",
    "                    \n",
    "                    for link in all_page_links:\n",
    "                        if link.text.strip() == str(next_page_num):\n",
    "                            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", link)\n",
    "                            time.sleep(0.5)\n",
    "                            driver.execute_script(\"arguments[0].click();\", link)\n",
    "                            print(f\"  üîò Clicked page {next_page_num}\")\n",
    "                            next_clicked = True\n",
    "                            time.sleep(4)\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    print(f\"  Method 4 failed: {e}\")\n",
    "            \n",
    "            if not next_clicked:\n",
    "                print(f\"  ‚ö†Ô∏è  Next page button not found, may have reached last page\")\n",
    "                break\n",
    "            \n",
    "            current_page += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Pagination operation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            break\n",
    "    \n",
    "    # Convert to list and sort\n",
    "    profile_urls = sorted(list(all_profile_urls))\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ Total found {len(profile_urls)} Faculty\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if len(profile_urls) == 0:\n",
    "        print(\"‚ùå No Faculty found\")\n",
    "        with open(\"directory_error.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(driver.page_source)\n",
    "        print(\"Saved page to directory_error.html\")\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    # Show first 20\n",
    "    print(\"\\nüìã First 20 Faculty:\")\n",
    "    for i, url in enumerate(profile_urls[:20], 1):\n",
    "        name = url.split('/profile/')[-1].replace('-', ' ').title()\n",
    "        print(f\"  {i}. {name}\")\n",
    "    \n",
    "    # =============== Step 3: Collect basic information for each Faculty ===============\n",
    "    print(f\"\\nüìå Step 3: Collecting detailed information for {len(profile_urls)} Faculty\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Ask whether to continue\n",
    "    user_input = input(f\"\\nThere are {len(profile_urls)} Faculty, estimated time {len(profile_urls)*2//60} minutes.\\nEnter 'y' to continue, or enter a number to collect only first N, or 'n' to save links only: \")\n",
    "    \n",
    "    if user_input.lower() == 'n':\n",
    "        # Only save links, don't visit detail pages\n",
    "        df_urls = pd.DataFrame({\n",
    "            'Profile_URL': profile_urls,\n",
    "            'Name_from_URL': [url.split('/profile/')[-1].replace('-', ' ').title() for url in profile_urls]\n",
    "        })\n",
    "        df_urls.to_csv(\"wisc_faculty_urls_only.csv\", index=False, encoding='utf-8-sig')\n",
    "        print(f\"‚úÖ Saved {len(profile_urls)} links to wisc_faculty_urls_only.csv\")\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    elif user_input.isdigit():\n",
    "        profile_urls = profile_urls[:int(user_input)]\n",
    "        print(f\"‚úì Will collect first {len(profile_urls)} Faculty\")\n",
    "    elif user_input.lower() != 'y':\n",
    "        print(\"Cancelled\")\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    faculty_list = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    for idx, profile_url in enumerate(profile_urls, 1):\n",
    "        try:\n",
    "            name_from_url = profile_url.split('/profile/')[-1].replace('-', ' ').title()\n",
    "            \n",
    "            print(f\"[{idx}/{len(profile_urls)}] {name_from_url}\")\n",
    "            \n",
    "            driver.get(profile_url)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Parse page\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Initialize data\n",
    "            info = {\n",
    "                'Name': '',\n",
    "                'First_Name': '',\n",
    "                'Last_Name': '',\n",
    "                'Title': '',\n",
    "                'Department': '',\n",
    "                'Email': '',\n",
    "                'Phone': '',\n",
    "                'Office': '',\n",
    "                'Profile_URL': profile_url,\n",
    "                'Google_Scholar_URL': ''\n",
    "            }\n",
    "            \n",
    "            # Extract name\n",
    "            name_tag = soup.find('h1')\n",
    "            if name_tag:\n",
    "                full_name = name_tag.get_text(strip=True)\n",
    "                info['Name'] = full_name\n",
    "                \n",
    "                name_parts = full_name.split()\n",
    "                if len(name_parts) >= 2:\n",
    "                    info['First_Name'] = name_parts[0]\n",
    "                    info['Last_Name'] = name_parts[-1]\n",
    "            else:\n",
    "                info['Name'] = name_from_url\n",
    "                name_parts = name_from_url.split()\n",
    "                if len(name_parts) >= 2:\n",
    "                    info['First_Name'] = name_parts[0]\n",
    "                    info['Last_Name'] = name_parts[-1]\n",
    "            \n",
    "            # Extract title\n",
    "            for tag in soup.find_all(['p', 'div', 'span', 'h2', 'h3']):\n",
    "                text = tag.get_text(strip=True)\n",
    "                if text and len(text) < 200:\n",
    "                    if any(word in text.lower() for word in ['professor', 'lecturer', 'instructor', 'assistant', 'associate']):\n",
    "                        info['Title'] = text\n",
    "                        break\n",
    "            \n",
    "            # Extract department\n",
    "            dept_elem = soup.find(['p', 'div', 'span'], class_=lambda x: x and 'department' in str(x).lower())\n",
    "            if dept_elem:\n",
    "                info['Department'] = dept_elem.get_text(strip=True)\n",
    "            \n",
    "            # Extract email\n",
    "            email_tag = soup.find('a', href=lambda x: x and 'mailto:' in x)\n",
    "            if email_tag:\n",
    "                info['Email'] = email_tag.get_text(strip=True).replace('mailto:', '')\n",
    "            \n",
    "            # Extract phone\n",
    "            phone_tag = soup.find('a', href=lambda x: x and 'tel:' in x)\n",
    "            if phone_tag:\n",
    "                info['Phone'] = phone_tag.get_text(strip=True)\n",
    "            \n",
    "            # Find Google Scholar link\n",
    "            scholar_link = soup.find('a', href=lambda x: x and 'scholar.google' in str(x))\n",
    "            if scholar_link:\n",
    "                info['Google_Scholar_URL'] = scholar_link.get('href')\n",
    "            \n",
    "            faculty_list.append(info)\n",
    "            \n",
    "            print(f\"  ‚úì {info['Name']}\")\n",
    "            print(f\"    {info['Title'][:50] if info['Title'] else 'N/A'}\")\n",
    "            \n",
    "            # Save every 20\n",
    "            if idx % 20 == 0:\n",
    "                backup_df = pd.DataFrame(faculty_list)\n",
    "                backup_df.to_csv(\"wisc_faculty_backup.csv\", index=False, encoding='utf-8-sig')\n",
    "                print(f\"\\n  üíæ Backed up {idx} members\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Failed: {e}\")\n",
    "            failed_count += 1\n",
    "            continue\n",
    "    \n",
    "    # =============== Step 4: Save data ===============\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìå Step 4: Saving data...\")\n",
    "    \n",
    "    df = pd.DataFrame(faculty_list)\n",
    "    \n",
    "    # Full information\n",
    "    df.to_csv(\"wisc_faculty_list_full.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(f\"‚úÖ wisc_faculty_list_full.csv ({len(df)} members)\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\nüìä Statistics:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total Faculty: {len(df)}\")\n",
    "    print(f\"Success: {len(df)}\")\n",
    "    print(f\"Failed: {failed_count}\")\n",
    "    print(f\"With Email: {(df['Email'] != '').sum()}\")\n",
    "    print(f\"With Google Scholar: {(df['Google_Scholar_URL'] != '').sum()}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All completed!\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è  User interrupted\")\n",
    "    if 'faculty_list' in locals() and faculty_list:\n",
    "        pd.DataFrame(faculty_list).to_csv(\"wisc_faculty_interrupted.csv\", index=False, encoding='utf-8-sig')\n",
    "        print(f\"üíæ Saved {len(faculty_list)} members\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    try:\n",
    "        driver.quit()\n",
    "        print(\"\\nüîí Browser closed\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25efcb-d2a3-41ef-8d80-34e8c964a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv(\"wisc_faculty_list_full.csv\", encoding='utf-8-sig')\n",
    "\n",
    "# Keep only Name and Profile_URL columns\n",
    "df_filtered = df[['Name', 'Profile_URL']]\n",
    "\n",
    "# Save as Excel\n",
    "df_filtered.to_excel(\"wisc_faculty_name_url.xlsx\", index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"‚úÖ Saved to wisc_faculty_name_url.xlsx\")\n",
    "print(f\"Total {len(df_filtered)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d86741f5-883c-484e-bf8c-2ef86db565a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name                                                        Profile_URL\n",
      "         Aaron Thielen         https://business.wisc.edu/directory/profile/aaron-thielen/\n",
      "        Abdullah Yavas        https://business.wisc.edu/directory/profile/abdullah-yavas/\n",
      "           Adam J Bock           https://business.wisc.edu/directory/profile/adam-j-bock/\n",
      "        Adam R Smedema        https://business.wisc.edu/directory/profile/adam-r-smedema/\n",
      "          Alan Stoffer          https://business.wisc.edu/directory/profile/alan-stoffer/\n",
      "Alexander D. Stajkovic https://business.wisc.edu/directory/profile/alexander-d-stajkovic/\n",
      "         Alina Arefeva         https://business.wisc.edu/directory/profile/alina-arefeva/\n",
      "              Allen Li              https://business.wisc.edu/directory/profile/allen-li/\n",
      "   Alyssa Gosbee Stang   https://business.wisc.edu/directory/profile/alyssa-gosbee-stang/\n",
      "          Amanda Kenny          https://business.wisc.edu/directory/profile/amanda-kenny/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read Excel file\n",
    "df = pd.read_excel(\"wisc_faculty_name_url.xlsx\")\n",
    "\n",
    "# Display first 10 rows as table\n",
    "print(df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6fbeda-6dd9-4409-8f05-b0610ddb9a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
